{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89deb5be-edcb-4777-88ce-aca5cb9e046a",
   "metadata": {
    "id": "89deb5be-edcb-4777-88ce-aca5cb9e046a"
   },
   "outputs": [],
   "source": [
    "# === Imports ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.layers import (Input, Conv1D, BatchNormalization, Activation, Add, GlobalAveragePooling1D, Dense, Dropout, MaxPooling1D, GaussianNoise\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import (accuracy_score, classification_report, mean_squared_error, mean_absolute_error, r2_score)\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "qsoRCuAtcERV",
   "metadata": {
    "id": "qsoRCuAtcERV"
   },
   "outputs": [],
   "source": [
    "# === Custom Dataset class for PyTorch ===\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = np.array(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = torch.tensor(self.data[index], dtype=torch.float32)\n",
    "        y = torch.tensor(self.label[index], dtype=torch.float32).unsqueeze(0)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390ae93e-4c78-4059-9d7f-b91a909c4cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyConv1dPadSame(nn.Module):\n",
    "    \"\"\"1D convolution with same padding.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, groups=1):\n",
    "        super().__init__()\n",
    "        self.stride = stride\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, stride, groups=groups)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_len = x.shape[-1]\n",
    "        out_len = (in_len + self.stride - 1) // self.stride\n",
    "        pad_total = max(0, (out_len - 1) * self.stride + self.kernel_size - in_len)\n",
    "        pad_left = pad_total // 2\n",
    "        pad_right = pad_total - pad_left\n",
    "        x = F.pad(x, (pad_left, pad_right))\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class MyMaxPool1dPadSame(nn.Module):\n",
    "    \"\"\"1D max pooling with same padding.\"\"\"\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.pool = nn.MaxPool1d(kernel_size=kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_len = x.shape[-1]\n",
    "        pad_total = max(0, self.kernel_size - 1)\n",
    "        pad_left = pad_total // 2\n",
    "        pad_right = pad_total - pad_left\n",
    "        x = F.pad(x, (pad_left, pad_right))\n",
    "        return self.pool(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bc2136-db5c-4286-ac1c-a476a9af11f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"Residual block with optional downsampling.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, groups, downsample, use_bn, use_do, is_first_block=False):\n",
    "        super().__init__()\n",
    "        self.downsample = downsample\n",
    "        self.out_channels = out_channels\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(in_channels) if use_bn else nn.Identity()\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.do1 = nn.Dropout(0.5) if use_do else nn.Identity()\n",
    "        self.conv1 = MyConv1dPadSame(in_channels, out_channels, kernel_size, stride if downsample else 1, groups)\n",
    "\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels) if use_bn else nn.Identity()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.do2 = nn.Dropout(0.5) if use_do else nn.Identity()\n",
    "        self.conv2 = MyConv1dPadSame(out_channels, out_channels, kernel_size, 1, groups)\n",
    "\n",
    "        self.pool = MyMaxPool1dPadSame(stride) if downsample else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.pool(x)\n",
    "        out = self.bn1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.do1(out)\n",
    "        out = self.conv1(out)\n",
    "\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.do2(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        if self.out_channels != self.in_channels:\n",
    "            identity = identity.transpose(1, 2)\n",
    "            pad = self.out_channels - self.in_channels\n",
    "            identity = F.pad(identity, (0, pad))\n",
    "            identity = identity.transpose(1, 2)\n",
    "\n",
    "        return out + identity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2b0079-a181-43e5-8d0e-53d763aa0a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet1D(nn.Module):\n",
    "    \"\"\"1D ResNet for regression.\"\"\"\n",
    "    def __init__(self, in_channels, base_filters, kernel_size, stride, groups, n_block,\n",
    "                 downsample_gap=2, increasefilter_gap=2, use_bn=True, use_do=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.first_conv = MyConv1dPadSame(in_channels, base_filters, kernel_size, stride=1)\n",
    "        self.first_bn = nn.BatchNorm1d(base_filters)\n",
    "        self.first_relu = nn.ReLU()\n",
    "\n",
    "        # Build residual blocks\n",
    "        self.blocks = nn.ModuleList()\n",
    "        filters = base_filters\n",
    "        in_ch = base_filters\n",
    "\n",
    "        for i in range(n_block):\n",
    "            if i != 0 and i % increasefilter_gap == 0:\n",
    "                filters *= 2\n",
    "\n",
    "            out_ch = filters\n",
    "            downsample = (i != 0 and i % downsample_gap == 0)\n",
    "\n",
    "            block = BasicBlock(\n",
    "                in_channels=in_ch,\n",
    "                out_channels=out_ch,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                groups=groups,\n",
    "                downsample=downsample,\n",
    "                use_bn=use_bn,\n",
    "                use_do=use_do,\n",
    "                is_first_block=(i == 0)\n",
    "            )\n",
    "            self.blocks.append(block)\n",
    "            in_ch = out_ch\n",
    "\n",
    "        # Final layers\n",
    "        self.final_bn = nn.BatchNorm1d(filters)\n",
    "        self.final_relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(filters, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_conv(x)\n",
    "        x = self.first_bn(x)\n",
    "        x = self.first_relu(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.final_bn(x)\n",
    "        x = self.final_relu(x)\n",
    "        x = x.mean(-1)  # Global average pooling\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "B5l9ji8Pb7hA",
   "metadata": {
    "id": "B5l9ji8Pb7hA"
   },
   "outputs": [],
   "source": [
    "def run_training(X_train, y_train, X_val, y_val, X_test, y_test, epochs=20):\n",
    "    \"\"\"\n",
    "    Trains the ResNet1D model for regression using MAE loss and early stopping.\n",
    "    \"\"\"\n",
    "\n",
    "    best_val_mae = float(\"inf\")\n",
    "    patience_counter = 5\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Convert numpy to float tensors\n",
    "    y_train, y_val, y_test = map(lambda arr: arr.astype(np.float32), [y_train, y_val, y_test])\n",
    "\n",
    "    train_loader = DataLoader(MyDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
    "    val_loader   = DataLoader(MyDataset(X_val, y_val), batch_size=64, shuffle=False)\n",
    "    test_loader  = DataLoader(MyDataset(X_test, y_test), batch_size=64, shuffle=False)\n",
    "\n",
    "    model = ResNet1D(\n",
    "        in_channels=X_train.shape[1],\n",
    "        base_filters=16,\n",
    "        kernel_size=16,\n",
    "        stride=1,\n",
    "        groups=1,\n",
    "        n_block=8,\n",
    "        downsample_gap=2,\n",
    "        increasefilter_gap=2,\n",
    "        use_bn=True,\n",
    "        use_do=False\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss, train_preds, train_targets = 0, [], []\n",
    "\n",
    "        for X, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\", leave=False):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            train_preds.extend(outputs.detach().cpu().numpy())\n",
    "            train_targets.extend(y.cpu().numpy())\n",
    "\n",
    "        # === Validation ===\n",
    "        model.eval()\n",
    "        val_loss, val_preds, val_targets = 0, [], []\n",
    "        with torch.no_grad():\n",
    "            for X, y in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\", leave=False):\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                outputs = model(X)\n",
    "                loss = criterion(outputs, y)\n",
    "                val_loss += loss.item()\n",
    "                val_preds.extend(outputs.cpu().numpy())\n",
    "                val_targets.extend(y.cpu().numpy())\n",
    "\n",
    "        # Convert predictions to arrays and inverse-transform\n",
    "        train_preds, val_preds = map(np.array, [train_preds, val_preds])\n",
    "        train_targets, val_targets = map(np.array, [train_targets, val_targets])\n",
    "\n",
    "        train_preds_rescaled = target_scaler.inverse_transform(train_preds.reshape(-1, 1)).flatten()\n",
    "        train_targets_rescaled = target_scaler.inverse_transform(train_targets.reshape(-1, 1)).flatten()\n",
    "        val_preds_rescaled = target_scaler.inverse_transform(val_preds.reshape(-1, 1)).flatten()\n",
    "        val_targets_rescaled = target_scaler.inverse_transform(val_targets.reshape(-1, 1)).flatten()\n",
    "\n",
    "        train_mae = mean_absolute_error(train_targets_rescaled, train_preds_rescaled)\n",
    "        val_mae = mean_absolute_error(val_targets_rescaled, val_preds_rescaled)\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        if val_mae < best_val_mae:\n",
    "            best_val_mae = val_mae\n",
    "            torch.save(model.state_dict(), \"best_resnet1d_model.pth\")\n",
    "            print(f\"New best model saved with Val MAE: {val_mae:.4f}\")\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= patience_counter:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Train MAE: {train_mae:.4f} | Val MAE: {val_mae:.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"resnet1d_model.pth\")\n",
    "    print(\"Model saved as 'resnet1d_model.pth'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9QBx5CrcLRb",
   "metadata": {
    "id": "b9QBx5CrcLRb"
   },
   "outputs": [],
   "source": [
    "train_csv, val_csv, test_csv = \"4x4_train.csv\", \"4x4_val.csv\", \"4x4_test.csv\"\n",
    "dfs = [pd.read_csv(f) for f in (train_csv, val_csv, test_csv)]\n",
    "\n",
    "# Drop unnecessary columns if present\n",
    "for df in dfs:\n",
    "    df.drop(columns=[\"apple_type\", \"hdr\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "# Split features/targets\n",
    "X_train, y_train = dfs[0].drop(columns=[\"apple_content\"]), dfs[0][\"apple_content\"]\n",
    "X_val,   y_val   = dfs[1].drop(columns=[\"apple_content\"]), dfs[1][\"apple_content\"]\n",
    "X_test,  y_test  = dfs[2].drop(columns=[\"apple_content\"]), dfs[2][\"apple_content\"]\n",
    "\n",
    "# === Scaling ===\n",
    "scaler = MinMaxScaler()\n",
    "X_train, X_val, X_test = map(lambda X: scaler.fit_transform(X) if X is X_train else scaler.transform(X), [X_train, X_val, X_test])\n",
    "\n",
    "target_scaler = MinMaxScaler()\n",
    "y_train = target_scaler.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "y_val   = target_scaler.transform(y_val.values.reshape(-1, 1)).flatten()\n",
    "y_test  = target_scaler.transform(y_test.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Reshape to (N, C, L)\n",
    "for arr in (X_train, X_val, X_test):\n",
    "    arr.shape = (arr.shape[0], 1, arr.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a014kRtVcqXb",
   "metadata": {
    "id": "a014kRtVcqXb"
   },
   "outputs": [],
   "source": [
    "# === Free up GPU memory ===\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4047b8eb-0352-492c-8184-dc88f8dfc17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training(X_train, y_train, X_val, y_val, X_test, y_test, epochs=150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "XLW4qiRGrNFw",
   "metadata": {
    "id": "XLW4qiRGrNFw"
   },
   "outputs": [],
   "source": [
    "model = ResNet1D(\n",
    "    in_channels=X_train.shape[1],\n",
    "    base_filters=16,\n",
    "    kernel_size=16,\n",
    "    stride=1,\n",
    "    groups=1,\n",
    "    n_block=8,\n",
    "    downsample_gap=2,\n",
    "    increasefilter_gap=2,\n",
    "    use_bn=True,\n",
    "    use_do=False\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"best_resnet1d_model.pth\"))\n",
    "test_loader = DataLoader(MyDataset(X_test, y_test), batch_size=16, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "test_preds, test_targets = [], []\n",
    "with torch.no_grad():\n",
    "    for X, y in test_loader:\n",
    "        X, y = X.to(device), y.to(device).unsqueeze(1)\n",
    "        outputs = model(X)\n",
    "        test_preds.extend(outputs.cpu().numpy())\n",
    "        test_targets.extend(y.cpu().numpy())\n",
    "\n",
    "test_preds = np.array(test_preds).squeeze()\n",
    "test_targets = np.array(test_targets).squeeze()\n",
    "\n",
    "test_preds_rescaled = target_scaler.inverse_transform(test_preds.reshape(-1, 1)).flatten()\n",
    "test_targets_rescaled = target_scaler.inverse_transform(test_targets.reshape(-1, 1)).flatten()\n",
    "\n",
    "test_rmse = np.sqrt(mean_squared_error(test_targets_rescaled, test_preds_rescaled))\n",
    "test_mae  = mean_absolute_error(test_targets_rescaled, test_preds_rescaled)\n",
    "test_r2   = r2_score(test_targets_rescaled, test_preds_rescaled)\n",
    "\n",
    "print(f\"RMSE: {test_rmse:.4f} | MAE: {test_mae:.4f} | R²: {test_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "R-NbmkUUcyQ0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R-NbmkUUcyQ0",
    "outputId": "4e72b3f4-1eda-428a-db8e-8667fd7a5158"
   },
   "outputs": [],
   "source": [
    "print(\"\\nSample Predictions vs. Actual Values:\")\n",
    "random_indices = random.sample(range(len(test_preds_rescaled)), 25)\n",
    "\n",
    "for i in random_indices:\n",
    "    print(f\"Target: {test_targets_rescaled[i]:.2f} | Predicted: {test_preds_rescaled[i]:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
